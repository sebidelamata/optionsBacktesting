---
output: html_document
#title: 'Gamma Gamma Hey (an options analysis) Part One/: Primary Data Collection and Cleaning'
layout: post
---

# Gamma Gamma Hey 

## (an options analysis) 

# Part One: Primary Data Collection and Cleaning




## Abstract

In this first post in a series analyzing real-world derivatives data, we set up a Raspberry Pi 4 with a R Shiny Server. R and shell scripts are used to create a table within the PostgreSQL database to hold the options data. Additional scripts and a cronjob are used to automate scraping data from the CBOE website and appending the data to the table in the PostgreSQL database. An R markdown is used to take an initial look at our data. 

## Background

I have been doing some thinking over the past months about how previous projects have been guilty of trying to cover too much ground at once. You take some data, you slap a model on it, voila! A true data project should be more than that. It should grow in time with your understanding of the data. To truly understand that data requires more than just a one-off project and a bit of research. It is with this intention that I aim to start this post as a series going through the entire lifecycle of a data project, from inception and data sourcing to advanced models and system maintenance. In this first post we will start our journey at a carte blanche. We have no fancy models. We have no data. We have nothing except a Raspberry Pi 4 and a curiosity about options.
The options market can be a (very dangerous) playground for a statistics nerd. Plenty of APIs can be found for finding stock data in R with some readily available libraries. Considerably less sources exist for options, but I am spoiled. I don't want Yahoo Finance data on an API someone else built. Why have room temperature Dasani when you can go drink from an Alpine spring? I want the source! Bring me to Pablo! This being said, the only rational conclusion is to scrape data directly from the CBOE ourselves. I don't just want this data for today, nor do I want the limited historical data being offered. Predictably, that will cost you. This is not a good start for an enterprise aimed primarily at not losing money! We may not be able to do this for past data (or tick-level data), but one solution is to create our own database, starting today.   

## Setting up the Raspberry Pi and R shiny Server

#### Hardware

The [Raspberry Pi](https://www.canakit.com/raspberry-pi-4-extreme-kit.html){target="_blank"} is the perfect tool to carry out our scraping needs. It's portable (currently hidden behind a night stand), uses very little power, I don't have to keep my main laptop on all the time, and we can dedicate it solely to our project.


```{r}


system(
  "cat /sys/firmware/devicetree/base/model",
  intern = TRUE
)

```


#### Memory

I am using a 128GB sd card to hold both the OS and all of our data (we will probably need to mount external storage eventually). For our operating system we want to choose the lightest possible while still being able to perform our task(s) at hand. We will be using Debian Bullseye.
 

```{r}


system(
  "df -h",
  intern = TRUE
)

```


#### Setting up the R Shiny Server

I followed [this guide](https://community.rstudio.com/t/setting-up-your-own-shiny-server-rstudio-server-on-a-raspberry-pi-3b/18982){target="_blank"} to install R shiny Server on a Raspberry Pi. There is an updated article that uses an Ansible build and would probably play much nicer with my 64-bit Raspberry Pi, but meh. The instructions work (mostly) and if you take enough stress-breaks you'll get there. The main work-around involves getting a 64-bit version of NGINX and node for arch instead of the 32-bit. Nothing too crazy. This does take about a day though, so make sure you bring snacks.


```{r}

version[c(2,13)]

installed.packages()[,3]

```

#### Before we proceed

A couple of things to keep in mind when setting this up. Our Raspberry Pi is good for scraping, but its not going to be training neural nets anytime soon. We just want to use this to grab data and store it. We can still use the Raspberry Pi to do some simple plots (and create this Rmarkdown), but we are probably going to do anything more in the way of heavy lifting on a more powerful machine.

## Setting up Postgresql

So now we have a situation where we will ideally be collecting an ever increasing amount of data. Saving this data as an Rdata object or a csv will quickly add up in terms of memory, and we would be stuck import all of the data before we could filter out what we wanted, which would slow down any analysis that uses it every time we want to call this object from memory. In addition, we previously stated that we wanted to be able to do more advanced models on other machines. Sending a large data file through SCP is tedious and tough to replicate at scale. The PostgreSQL is a light RDBMS that is already included in our Shiny build, so let's just use that. We will be using the public schema, but be sure to set up a db before proceeding. R has many packages that will allow users to connect to a PostgreSQL database. We will be using RPostgreSQL and DBI, but there are others.

## Scraping options data from the CBOE

We can scrape data directly from the CBOE delayed quotes page. [Here is an example](https://www.cboe.com/delayed_quotes/spy/quote_table){target="_blank"} of the SPY option chain. This provides plenty of info, including calculating the Greeks for us. Unfortunately, the CBOE uses scripts on their page to prevent scrapers from getting their data. Fortunately, the internet exists. [Here is a link](https://www.youtube.com/watch?v=AyJInEA6-wo){target="_blank"} showing how to do this specifically to the CBOE. [Here is another link](https://www.youtube.com/watch?v=WPh6yuCQHBQ){target="_blank"} on how to scrape data from sites running scripts. Go nuts.

### Scraping a single ticker (scraper.R)

To start we only need the `jsonlite` package to scrape, the others are for manipulation and cleaning.

```{r}
# import our libraries
library(dplyr)
library(jsonlite)
library(stringi)

```

We create a variable called `ticker` to hold our desired value, then supply it to the string to make our ticker request.

```{r}

# let's pick a ticker to scrape
ticker <- "QQQ"

# create our scraping address
scrape_target <- paste0(
  "https://cdn.cboe.com/api/global/delayed_quotes/options/",
  ticker,
  ".json"
  )
  
print(scrape_target)

```

This will fetch data for every strike for every expiration for that ticker.

```{r}
# next we read the json data at our scrape target
scrape_data <- read_json(
    scrape_target,
    simplifyVector = TRUE
  )

```

This returns a `list` object to us (think of it as `R`'s best approximation for `json` data nested structure).

```{r}

typeof(scrape_data)

```

Let's take a quick look at where most of our useful data is housed.

```{r}

glimpse(scrape_data$data$options)

```

This is full of information, but we need to transform this into a dataframe so we can then store this into a similarly structured PostgreSQL database.

```{r}

# make it into a data frame
option_data <- as.data.frame(
  scrape_data$data$options
)

```

We also need to change the datetime to an R datetime object.

```{r}

# clean last trade datetime from string
option_data$last_trade_time <- as.POSIXct(
  option_data$last_trade_time, 
  "%Y-%m-%dT%H:%M:%S", 
  tz = "America/Chicago"
  )
```

We can also grab some useful data about the underlying from a different part of the nested data.

```{r}

# grab umderlying close
option_data$underlying_close <- scrape_data$data$close

```

Let's take a look at the collumn of our data.

```{r}

print(option_data$option[1])

```

This doesn't look all that informative on the surface, but this actually has a lot of information. If you look at the above, `QQQ` represents the underlying `22` represents the year of expiration, `06` the month, `13` the day, `c` whether it is a call or a put, and `00230000` represents the $230.00 strike. Let's create some more useful columns from this.


```{r}
  # grab our strike price from the option name string
  option_data$strike_price <- as.numeric(stri_sub(option_data$option, -8)) / 1000
   
  # grab our contract type from the option name string
  option_data$contract_type <- as.factor(stri_sub(option_data$option, -9, -9))
  
  # grab our underlying ticker from the option name string
  option_data$underlying_ticker <- as.factor(stri_sub(option_data$option, -999, -16))
  
  # grab our underlying ticker from the option name string
  option_data$expiration_date <- as.Date(
    stri_sub(option_data$option, -15, -10),
    format = "%y%m%d"
    )
```

Options data is unsurpisingly, not static. The value for the 330 call will be different (probably) tomorrow than it is today. We need to grab the date on the date that we scrape the data so we can tell when this data was taken. Let's create a column for that.

```{r}

# if we want to set this up to scrape every day
# we need to create a column to record 
# on what day the data was scraped
option_data$scrape_date <- Sys.Date()

```

We can also use that original string from the first column as the primary key column in our PostgreSQL database. To make this our unique identifier, we append the date of the scrape to the end of the string to make each row unique based on a string combination of the underlying, expiration, strike, and scrape date.

```{r}

# since we have already taken all the useful data
# from the option column, we can keep using it as
# a unique identifier for our table if we append
# the scrape date to the end of the string
option_data$option <- paste0(
  option_data$option,
  as.character(Sys.Date())
)
  
print(option_data[1,])

```

### Iterating our scraper over our watchlist (grab_watchlist.R)

```{r}

# grab our scraper script
source("/home/sebi/optionsBacktesting/scraper.R")

```

```{r}


  # declare the tickers we want in our watchlist
watchlist <- c(
  "QQQ",
  "SPY",
  "IWM",
  "SLYV",
  "FXI",
  "DIA",
  "ARKK",
  "FEZ",
  "EEM",
  "EWW",
  "EWZ",
  "XLB",
  "XLV",
  "XLU",
  "XLF",
  "XLI",
  "XOP",
  "GLD",
  "SLV",
  "TLT",
  "HYG"
)

```
  
```{r}
  # create an empty dataframe 
  watchlist_data <- data.frame()

  summary(watchlist_data)
  
```

```{r}
  # for each ticker in the watchlist grab the option data
  # and union it to the watchlist_data df
  watchlist_data <- do.call(
    rbind,
    lapply(
      watchlist,
      grab_option_data
      )
    )

print(unique(as.factor(watchlist_data$underlying_ticker)))
```

```{r}
# number of rows for a single day's pull
nrow(watchlist_data)

```

### Creating a table in PostgreSQL (create_watchlist_table.R)

```{r}

library(RPostgreSQL)

source("/home/sebi/optionsBacktesting/grab_watchlist.R")
```

```{r eval=FALSE}

# set driver name
driver_name <- dbDriver(drvName = "PostgreSQL")
  
# establish database connection
db <- DBI::dbConnect(driver_name,
		     dbname="sebi",
		     host="localhost",
		     port = 5432
		     )
```

```{r eval=FALSE}

# grab todays watchlist data
watchlist_data <- grab_watchlist()

```  
  
```{r eval=FALSE}

# create our table with todays data (overwrite if already exists)
DBI::dbWriteTable(
  db,
  value =  watchlist_data,
  name = "watchlist_data",
  overwrite = TRUE,
  row.names = FALSE
)
```


```{r eval=FALSE}  
# set primary key column
DBI::dbSendQuery(
    db,
    'ALTER TABLE watchlist_data ADD PRIMARY KEY ("option")'
    )
```

```{r eval=FALSE}
  # disconnect from database
  DBI::dbDisconnect(db)

```

### Appending to the existing PostgreSQL table (append_watchlist_data.R)

```{r}

library(DBI)
library(RPostgreSQL)

source("/home/sebi/optionsBacktesting/grab_watchlist.R")

```

```{r eval=FALSE}

append_watchlist_data <- function(){
  
  # grab our watchlist data
  watchlist_data <- grab_watchlist()
  
  # establish driver name
  driver_name <- dbDriver(drvName = "PostgreSQL")
  
  # create database connection
  db <- DBI::dbConnect(driver_name,
                       dbname="sebi",
                       host="localhost",
                       port = 5432
  )
  
  # append our scraped data to the table
  DBI::dbWriteTable(
    db,
    name = "watchlist_data",
    value = watchlist_data,
    row.names = FALSE,
    append = TRUE
  )
  
  # close database connection
  DBI::dbDisconnect(db)
}

```

### Only scrape for data on market days (trading_day_scheduler.R)

```{r}

library(timeDate)

source("/home/sebi/optionsBacktesting/append_watchlist_data.R")

if(as.POSIXlt(Sys.Date())$wday %in% 1:5 & !(Sys.Date() %in% as.Date(timeDate::holidayNYSE()))){
  append_watchlist_data()
} else {
  message("Market Closed Today")
}

```

### Setting up a cronjob to automate data scrapes

```{r}

system(
  "sudo crontab -u sebi -l",
  intern = TRUE
)

```

```{r}


system(
  "ls -al /home/sebi/optionsBacktesting/",
  intern = TRUE
)

```

### Performing an intial query from our database

```{r}

library(getPass)
library(ggplot2)
library(lubridate)
library(reshape2)
library(treemap)
library(wesanderson)
library(RColorBrewer)

# set driver name
driver_name <- dbDriver(drvName = "PostgreSQL")

# establish database connection
db <- DBI::dbConnect(driver_name,
                     dbname = "sebi",
                     host = "192.168.0.12",
                     port = 5432,
                     user = "sebi",
                     password = getPass("Enter Password:")
)

res <- dbSendQuery(db, "SELECT * FROM watchlist_data;")
data_pull <- dbFetch(res)
dbClearResult(res)
dbDisconnect(db)

```

```{r}
# make a vector of the tickers from the data fetch
tickers <- as.character(unique(as.factor(data_pull$underlying_ticker)))

# grab just the options from the most recent pull
recent_pull <- data_pull[data_pull$scrape_date == max(data_pull$scrape_date),]

# a blank dataframe to hold our put call ratios
pcr_list <- data.frame(matrix(ncol = length(tickers), nrow = 1))
colnames(pcr_list) <- tickers

print(pcr_list)

```

```{r}

# calculate put call ratio for each ticker and add it to the put call ratio data frame
lapply(
  1:length(tickers),
  function(x){
    column_name <- tickers[x]
    if(sum(recent_pull[recent_pull$underlying_ticker == column_name & recent_pull$contract_type == "C","volume"]) != 0){
      pcr_list[,column_name] <<- sum(recent_pull[recent_pull$underlying_ticker == column_name & recent_pull$contract_type == "P","volume"]) / sum(recent_pull[recent_pull$underlying_ticker == column_name & recent_pull$contract_type == "C","volume"])
    } else {
        pcr_list[,column_name] <<- NA
      }
  }
)

print(pcr_list)

```

```{r}


# melt our data for our plot
melted_pcr_list <- reshape2::melt(
  pcr_list[, colSums(is.na(pcr_list)) < nrow(pcr_list)],
  variable.name = "Underlying",
  value.name = "put_call_ratio"
)

# turn the underlying ticker into a factor
melted_pcr_list$Underlying <- factor(
  melted_pcr_list$Underlying, 
  levels = melted_pcr_list$Underlying[order(melted_pcr_list$put_call_ratio)]
  )

print(melted_pcr_list)

```

```{r}

ggplot(
  data =melted_pcr_list,
  aes(
    y = Underlying,
    x = put_call_ratio,
    fill = Underlying
  )
) +
  geom_bar(stat = "identity") +
  ggtitle("Watchlist Put-Call Ratios") +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_fill_manual(values = colorRampPalette(wes_palettes$Rushmore1)(nrow(melted_pcr_list))) +
  xlab("Put Call Ratio")

put_call_ratio_tmap <- treemap(
  dtf = melted_pcr_list,
  index = c("Underlying", "put_call_ratio"),
  vSize = "put_call_ratio",
  vColor = "put_call_ratio",
  type = "index",
  align.labels = list(
    c("center", "center"),
    c("left", "bottom")
  ),
  palette = "Reds",
  title = "Watchlist Put-Call Ratios"
) 
  
  

```
