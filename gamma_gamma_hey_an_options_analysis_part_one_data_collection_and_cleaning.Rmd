---
output: html_document
#title: 'Gamma Gamma Hey (an options analysis) Part One/: Primary Data Collection and Cleaning'
layout: post
---

# Gamma Gamma Hey 

## (an options analysis) 

# Part One: Primary Data Collection and Cleaning




## Abstract

In this first post in a series analyzing real-world derivatives data, we set up a Raspberry Pi 4 with a R Shiny Server. R and shell scripts are used to create a table within the PostgreSQL database to hold the options data. Additional scripts and a cronjob are used to automate scraping data from the CBOE website and appending the data to the table in the PostgreSQL database. An R markdown is used to take an initial look at our data. 

## Background

I have been doing some thinking over the past months about how previous projects have been guilty of trying to cover too much ground at once. You take some data, you slap a model on it, voila! A true data project should be more than that. It should grow in time with your understanding of the data. To truly understand that data requires more than just a one-off project and a bit of research. It is with this intention that I aim to start this post as a series going through the entire lifecycle of a data project, from inception and data sourcing to advanced models and system maintenance. In this first post we will start our journey at a carte blanche. We have no fancy models. We have no data. We have nothing except a Raspberry Pi 4 and a curiosity about options.
The options market can be a (very dangerous) playground for a statistics nerd. Plenty of APIs can be found for finding stock data in R with some readily available libraries. Considerably less sources exist for options, but I am spoiled. I don't want Yahoo Finance data on an API someone else built. Why have room temperature Dasani when you can go drink from an Alpine spring? I want the source! Bring me to Pablo! This being said, the only rational conclusion is to scrape data directly from the CBOE ourselves. I don't just want this data for today, nor do I want the limited historical data being offered. Predictably, that will cost you. This is not a good start for an enterprise aimed primarily at not losing money! We may not be able to do this for past data (or tick-level data), but one solution is to create our own database, starting today.   

## Setting up the Raspberry Pi and R shiny Server

The [Raspberry Pi](https://www.canakit.com/raspberry-pi-4-extreme-kit.html){target="_blank"} is the perfect tool to carry out our scraping needs. It's portable (currently hidden behind a night stand), uses very little power, I don't have to keep my main laptop on all the time, and we can dedicate it solely to our project.

```{r}


system(
  "cat /sys/firmware/devicetree/base/model",
  intern = TRUE
)

```

 I am using a 128GB sd card to hold both the OS and all of our data (we will probably need to mount external storage eventually). For our operating system we want to choose the lightest possible while still being able to perform our task(s) at hand. We will be using Debian Bullseye.
 
```{r}


system(
  "df -h",
  intern = TRUE
)

```
 


	I followed [this guide](https://community.rstudio.com/t/setting-up-your-own-shiny-server-rstudio-server-on-a-raspberry-pi-3b/18982){target="_blank"} to install R shiny Server on a Raspberry Pi. There is an updated article that uses an Ansible build and would probably play much nicer with my 64-bit Raspberry Pi... but meh. The instructions work (mostly) and if you take enough stress-breaks you'll get there. The main work-around involves getting a 64-bit version of NGINX and node for arch instead of the 32-bit. Nothing too crazy. This does take about a day though, so make sure you bring snacks.



```{r}

version[c(2,13)]

installed.packages()[,3]

```

	A couple of things to keep in mind when setting this up. Our Raspberry Pi is good for scraping, but its not going to be training neural nets anytime soon. We just want to use this to grab data and store it. We can still use the Raspberry Pi to do some simple plots (and create this Rmarkdown), but we are probably going to do anything more heavy lifting on a more powerful machine.

## Setting up R and Postgresql



## Scraping options data from the CBOE

### Scraping a single ticker (scraper.R)

```{r}
# import our libraries
library(dplyr)
library(jsonlite)
library(stringi)

```

```{r}

# let's pick a ticker to scrape
ticker <- "QQQ"

# create our scraping address
scrape_target <- paste0(
  "https://cdn.cboe.com/api/global/delayed_quotes/options/",
  ticker,
  ".json"
  )
  
print(scrape_target)

```

```{r}
# next we read the json data at our scrape target
scrape_data <- read_json(
    scrape_target,
    simplifyVector = TRUE
  )

```


```{r}

typeof(scrape_data)

```


```{r}

glimpse(scrape_data$data$options)

```

```{r}

# make it into a data frame
option_data <- as.data.frame(
  scrape_data$data$options
)

  # clean last trade datetime from string
  option_data$last_trade_time <- as.POSIXct(
    option_data$last_trade_time, 
    "%Y-%m-%dT%H:%M:%S", 
    tz = "America/Chicago"
    )
  
  # grab umderlying close
  option_data$underlying_close <- scrape_data$data$close
```
  
```{r}

print(option_data$option[1])

```
  
```{r}
  # grab our strike price from the option name string
  option_data$strike_price <- as.numeric(stri_sub(option_data$option, -8)) / 1000
   
  # grab our contract type from the option name string
  option_data$contract_type <- as.factor(stri_sub(option_data$option, -9, -9))
  
  # grab our underlying ticker from the option name string
  option_data$underlying_ticker <- as.factor(stri_sub(option_data$option, -999, -16))
  
  # grab our underlying ticker from the option name string
  option_data$expiration_date <- as.Date(
    stri_sub(option_data$option, -15, -10),
    format = "%y%m%d"
    )
```

```{r}

  # if we want to set this up to scrape every day
  # we need to create a column to record 
  # on what day the data was scraped
  option_data$scrape_date <- Sys.Date()
  
  # since we have already taken all the useful data
  # from the option column, we can keep using it as
  # a unique identifier for our table if we append
  # the scrape date to the end of the string
  option_data$option <- paste0(
    option_data$option,
    as.character(Sys.Date())
  )
  
print(option_data[1,])

```

### Iterating our scraper over our watchlist (grab_watchlist.R)

```{r}

# grab our scraper script
source("/home/sebi/optionsBacktesting/scraper.R")

```

```{r}


  # declare the tickers we want in our watchlist
watchlist <- c(
  "QQQ",
  "SPY",
  "IWM",
  "SLYV",
  "FXI",
  "DIA",
  "ARKK",
  "FEZ",
  "EEM",
  "EWW",
  "EWZ",
  "XLB",
  "XLV",
  "XLU",
  "XLF",
  "XLI",
  "XOP",
  "GLD",
  "SLV",
  "TLT",
  "HYG"
)

```
  
```{r}
  # create an empty dataframe 
  watchlist_data <- data.frame()

  summary(watchlist_data)
  
```

```{r}
  # for each ticker in the watchlist grab the option data
  # and union it to the watchlist_data df
  watchlist_data <- do.call(
    rbind,
    lapply(
      watchlist,
      grab_option_data
      )
    )

print(unique(as.factor(watchlist_data$underlying_ticker)))
```

```{r}
# number of rows for a single day's pull
nrow(watchlist_data)

```

### Creating a table in PostgreSQL (create_watchlist_table.R)

```{r}

library(RPostgreSQL)

source("/home/sebi/optionsBacktesting/grab_watchlist.R")
```

```{r eval=FALSE}

# set driver name
driver_name <- dbDriver(drvName = "PostgreSQL")
  
# establish database connection
db <- DBI::dbConnect(driver_name,
		     dbname="sebi",
		     host="localhost",
		     port = 5432
		     )
```

```{r eval=FALSE}

# grab todays watchlist data
watchlist_data <- grab_watchlist()

```  
  
```{r eval=FALSE}

# create our table with todays data (overwrite if already exists)
DBI::dbWriteTable(
  db,
  value =  watchlist_data,
  name = "watchlist_data",
  overwrite = TRUE,
  row.names = FALSE
)
```


```{r eval=FALSE}  
# set primary key column
DBI::dbSendQuery(
    db,
    'ALTER TABLE watchlist_data ADD PRIMARY KEY ("option")'
    )
```

```{r eval=FALSE}
  # disconnect from database
  DBI::dbDisconnect(db)

```

### Appending to the existing PostgreSQL table (append_watchlist_data.R)

```{r}

library(DBI)
library(RPostgreSQL)

source("/home/sebi/optionsBacktesting/grab_watchlist.R")

```

```{r eval=FALSE}

append_watchlist_data <- function(){
  
  # grab our watchlist data
  watchlist_data <- grab_watchlist()
  
  # establish driver name
  driver_name <- dbDriver(drvName = "PostgreSQL")
  
  # create database connection
  db <- DBI::dbConnect(driver_name,
                       dbname="sebi",
                       host="localhost",
                       port = 5432
  )
  
  # append our scraped data to the table
  DBI::dbWriteTable(
    db,
    name = "watchlist_data",
    value = watchlist_data,
    row.names = FALSE,
    append = TRUE
  )
  
  # close database connection
  DBI::dbDisconnect(db)
}

```

### Only scrape for data on market days (trading_day_scheduler.R)

```{r}

library(timeDate)

source("/home/sebi/optionsBacktesting/append_watchlist_data.R")

if(as.POSIXlt(Sys.Date())$wday %in% 1:5 & !(Sys.Date() %in% as.Date(timeDate::holidayNYSE()))){
  append_watchlist_data()
} else {
  message("Market Closed Today")
}

```

### Setting up a cronjob to automate data scrapes

```{r}

system(
  "sudo crontab -u sebi -l",
  intern = TRUE
)

```

```{r}


system(
  "ls -al /home/sebi/optionsBacktesting/",
  intern = TRUE
)

```

### Performing an intial query from our database

```{r}

library(getPass)
library(ggplot2)
library(lubridate)
library(reshape2)
library(treemap)
library(wesanderson)
library(RColorBrewer)

# set driver name
driver_name <- dbDriver(drvName = "PostgreSQL")

# establish database connection
db <- DBI::dbConnect(driver_name,
                     dbname = "sebi",
                     host = "192.168.0.12",
                     port = 5432,
                     user = "sebi",
                     password = getPass("Enter Password:")
)

res <- dbSendQuery(db, "SELECT * FROM watchlist_data;")
data_pull <- dbFetch(res)
dbClearResult(res)
dbDisconnect(db)

```

```{r}
# make a vector of the tickers from the data fetch
tickers <- as.character(unique(as.factor(data_pull$underlying_ticker)))

# grab just the options from the most recent pull
recent_pull <- data_pull[data_pull$scrape_date == max(data_pull$scrape_date),]

# a blank dataframe to hold our put call ratios
pcr_list <- data.frame(matrix(ncol = length(tickers), nrow = 1))
colnames(pcr_list) <- tickers

print(pcr_list)

```

```{r}

# calculate put call ratio for each ticker and add it to the put call ratio data frame
lapply(
  1:length(tickers),
  function(x){
    column_name <- tickers[x]
    if(sum(recent_pull[recent_pull$underlying_ticker == column_name & recent_pull$contract_type == "C","volume"]) != 0){
      pcr_list[,column_name] <<- sum(recent_pull[recent_pull$underlying_ticker == column_name & recent_pull$contract_type == "P","volume"]) / sum(recent_pull[recent_pull$underlying_ticker == column_name & recent_pull$contract_type == "C","volume"])
    } else {
        pcr_list[,column_name] <<- NA
      }
  }
)

print(pcr_list)

```

```{r}


# melt our data for our plot
melted_pcr_list <- reshape2::melt(
  pcr_list[, colSums(is.na(pcr_list)) < nrow(pcr_list)],
  variable.name = "Underlying",
  value.name = "put_call_ratio"
)

# turn the underlying ticker into a factor
melted_pcr_list$Underlying <- factor(
  melted_pcr_list$Underlying, 
  levels = melted_pcr_list$Underlying[order(melted_pcr_list$put_call_ratio)]
  )

print(melted_pcr_list)

```

```{r}

ggplot(
  data =melted_pcr_list,
  aes(
    y = Underlying,
    x = put_call_ratio,
    fill = Underlying
  )
) +
  geom_bar(stat = "identity") +
  ggtitle("Watchlist Put-Call Ratios") +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_fill_manual(values = colorRampPalette(wes_palettes$Rushmore1)(nrow(melted_pcr_list))) +
  xlab("Put Call Ratio")

put_call_ratio_tmap <- treemap(
  dtf = melted_pcr_list,
  index = c("Underlying", "put_call_ratio"),
  vSize = "put_call_ratio",
  vColor = "put_call_ratio",
  type = "index",
  align.labels = list(
    c("center", "center"),
    c("left", "bottom")
  ),
  palette = "Reds",
  title = "Watchlist Put-Call Ratios"
) 
  
  

```
