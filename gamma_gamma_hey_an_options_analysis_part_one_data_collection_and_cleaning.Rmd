---
output: html_document
#title: 'Gamma Gamma Hey (an options analysis) Part One/: Primary Data Collection and Cleaning'
layout: post
---

# Gamma Gamma Hey 

## (an options analysis) 

# Part One: Primary Data Collection and Cleaning




## Abstract

In this first post in a series analyzing real-world derivatives data, we set up a Raspberry Pi 4 with a R Shiny Server. R and shell scripts are used to create a table within the PostgreSQL database to hold the options data. Additional scripts and a `cron` job are used to automate scraping data from the CBOE website and appending the data to the table in the PostgreSQL database. An R markdown is used to take an initial look at our data. 

## Background

I have been doing some thinking over the past months about how my previous projects have been guilty of trying to cover too much ground at once. You take some data, you slap a model on it, voila! A true data project should be more than that. It should grow in time with your understanding of the data. To truly understand that data requires more than just a one-off project and a bit of research. It is with this intention that I aim to start this post as a series going through the entire lifecycle of a data project, from inception and data sourcing to advanced models and system maintenance. In this first post we will start our journey at a carte blanche. We have no fancy models. We have no data. We have nothing except a Raspberry Pi 4 and a curiosity about options.
The options market can be a (very dangerous) playground for a statistics nerd. Plenty of APIs can be found for finding stock data in R with some readily available libraries. Considerably less sources exist for options, but I am spoiled. I don't want Yahoo Finance data on an API someone else built. Why have room temperature Dasani when you can go drink from an Alpine spring? I want the source! Bring me to Pablo! This being said, the only rational conclusion is to scrape data directly from the CBOE ourselves. I don't just want this data for today, nor do I want the limited historical data being offered. Predictably, that will cost you. This is not a good start for an enterprise aimed primarily at not losing money! We may not be able to do this for past data (or tick-level data), but one solution is to create our own database, starting today.   

## Setting up the Raspberry Pi and R shiny Server

#### Hardware

The [Raspberry Pi](https://www.canakit.com/raspberry-pi-4-extreme-kit.html){target="_blank"} is the perfect tool to carry out our scraping needs. It's portable (currently hidden behind a night stand), uses very little power, I don't have to keep my main laptop on all the time, and we can dedicate it solely to our project.


```{r}


system(
  "cat /sys/firmware/devicetree/base/model",
  intern = TRUE
)

```


#### Memory

I am using a 128GB sd card to hold both the OS and all of our data (we will probably need to mount external storage eventually). For our operating system we want to choose the lightest possible while still being able to perform our task(s) at hand. We will be using Debian Bullseye.
 

```{r}


system(
  "df -h",
  intern = TRUE
)

```


#### Setting up the R Shiny Server

I followed [this guide](https://community.rstudio.com/t/setting-up-your-own-shiny-server-rstudio-server-on-a-raspberry-pi-3b/18982){target="_blank"} to install R shiny Server on a Raspberry Pi. There is an updated article that uses an Ansible build and would probably play much nicer with my 64-bit Raspberry Pi, but meh. The instructions work (mostly) and if you take enough stress-breaks you'll get there. The main work-around involves getting a 64-bit version of NGINX and node for arch instead of the 32-bit. Nothing too crazy. This does take about a day though, so make sure you bring snacks.


```{r}

version[c(2,13)]

installed.packages()[,3]

```

#### Before we proceed

A couple of things to keep in mind when setting this up. Our Raspberry Pi is good for scraping, but its not going to be training neural nets anytime soon. We just want to use this to grab data and store it. We can still use the Raspberry Pi to do some simple plots (and create this Rmarkdown), but we are probably going to do anything more in the way of heavy lifting on a more powerful machine.

## Setting up PostgreSQL

So now we have a situation where we will ideally be collecting an ever increasing amount of data. Saving this data as an `.Rdata` object or a `.csv` will quickly add up in terms of memory, and we would be stuck import all of the data before we could filter out what we wanted, which would slow down any analysis that uses it every time we want to call this object from memory. In addition, we previously stated that we wanted to be able to do more advanced models on other machines. Sending a large data file through SCP is tedious and tough to replicate at scale. The PostgreSQL is a light RDBMS that is already included in our Shiny build, so let's just use that. We will be using the public schema, but be sure to set up a `db` before proceeding. R has many packages that will allow users to connect to a PostgreSQL database. We will be using `RPostgreSQL` and `DBI`, but there are others.

## Scraping options data from the CBOE

We can scrape data directly from the CBOE delayed quotes page. [Here is an example](https://www.cboe.com/delayed_quotes/spy/quote_table){target="_blank"} of the SPY option chain. This provides plenty of info, including calculating the Greeks for us. Unfortunately, the CBOE uses scripts on their page to prevent scrapers from getting their data. Fortunately, the internet exists. [Here is a link](https://www.youtube.com/watch?v=AyJInEA6-wo){target="_blank"} showing how to do this specifically to the CBOE. [Here is another link](https://www.youtube.com/watch?v=WPh6yuCQHBQ){target="_blank"} on how to scrape data from sites running scripts. Go nuts.

### Scraping a single ticker (scraper.R)

To start we only need the `jsonlite` package to scrape, the others are for manipulation and cleaning.

```{r}
# import our libraries
library(dplyr)
library(jsonlite)
library(stringi)

```

We create a variable called `ticker` to hold our desired value, then supply it to the string to make our ticker request.

```{r}

# let's pick a ticker to scrape
ticker <- "QQQ"

# create our scraping address
scrape_target <- paste0(
  "https://cdn.cboe.com/api/global/delayed_quotes/options/",
  ticker,
  ".json"
  )
  
print(scrape_target)

```

This will fetch data for every strike for every expiration for that ticker.

```{r}
# next we read the json data at our scrape target
scrape_data <- read_json(
    scrape_target,
    simplifyVector = TRUE
  )

```

This returns a `list` object to us (think of it as `R`'s best approximation for `json` data nested structure).

```{r}

typeof(scrape_data)

```

Let's take a quick look at where most of our useful data is housed.

```{r}

glimpse(scrape_data$data$options)

```

This is full of information, but we need to transform this into a dataframe so we can then store this into a similarly structured PostgreSQL database.

```{r}

# make it into a data frame
option_data <- as.data.frame(
  scrape_data$data$options
)

```

We also need to change the datetime to an R datetime object.

```{r}

# clean last trade datetime from string
option_data$last_trade_time <- as.POSIXct(
  option_data$last_trade_time, 
  "%Y-%m-%dT%H:%M:%S", 
  tz = "America/Chicago"
  )
```

We can also grab some useful data about the underlying from a different part of the nested data.

```{r}

# grab underlying close
option_data$underlying_close <- scrape_data$data$close

```

Let's take a look at the column of our data.

```{r}

print(option_data$option[1])

```

This doesn't look all that informative on the surface, but this actually has a lot of information. If you look at the above, `QQQ` represents the underlying, `22` represents the year of expiration, `06` the month, `13` the day, `c` whether it is a call or a put, and `00230000` represents the $230.00 strike. Let's create some more useful columns from this.


```{r}
  # grab our strike price from the option name string
  option_data$strike_price <- as.numeric(stri_sub(option_data$option, -8)) / 1000
   
  # grab our contract type from the option name string
  option_data$contract_type <- as.factor(stri_sub(option_data$option, -9, -9))
  
  # grab our underlying ticker from the option name string
  option_data$underlying_ticker <- as.factor(stri_sub(option_data$option, -999, -16))
  
  # grab our underlying ticker from the option name string
  option_data$expiration_date <- as.Date(
    stri_sub(option_data$option, -15, -10),
    format = "%y%m%d"
    )
```

Options data is unsurpisingly, not static. The value for the $330 call will be different (probably) tomorrow than it is today. We need to grab the date on the date that we scrape the data so we can tell when this data was taken. Let's create a column for that.

```{r}

# if we want to set this up to scrape every day
# we need to create a column to record 
# on what day the data was scraped
option_data$scrape_date <- Sys.Date()

```

We can also use that original string from the first column as the primary key column in our PostgreSQL database. To make this our unique identifier, we append the date of the scrape to the end of the string to make each row unique based on a string combination of the underlying, expiration, strike, and scrape date.

```{r}

# since we have already taken all the useful data
# from the option column, we can keep using it as
# a unique identifier for our table if we append
# the scrape date to the end of the string
option_data$option <- paste0(
  option_data$option,
  as.character(Sys.Date())
)

```

Let's take a look at our cleaned data. We can add more columns later, but this will be what will be stored in the PostgreSQL database.

```{r}

print(option_data[1,])

```

### Iterating our scraper over our watchlist (grab_watchlist.R)

Scraping and storing data is nice, but I want to expand this to get all of the tickers on my watchlist. First we call the previous `scraper.R` file that defines our scraper function.

```{r}

# grab our scraper script
source("/home/sebi/optionsBacktesting/scraper.R")

```

Due to restrictions from my job, these are all broad market ETFs. SLYV is notably less liquid than the others, but I sell covered calls against it, so why not include it too.

```{r}


  # declare the tickers we want in our watchlist
watchlist <- c(
  "QQQ",
  "SPY",
  "IWM",
  "SLYV",
  "FXI",
  "DIA",
  "ARKK",
  "FEZ",
  "EEM",
  "EWW",
  "EWZ",
  "XLB",
  "XLV",
  "XLU",
  "XLF",
  "XLI",
  "XOP",
  "GLD",
  "SLV",
  "TLT",
  "HYG"
)

```
  
We will want to create an empty dataframe where we will append the results of a call to `scraper.R` for each ticker.  

```{r}
  # create an empty dataframe 
  watchlist_data <- data.frame()

  summary(watchlist_data)
  
```

This simply iterates through our watchlist we created, and for every ticker we run `grab_option_data()` (the function we defined in `scraper.R`). This will return a dataframe, which we will then stick onto the bottom of our formerly empty data frame.

```{r}
  # for each ticker in the watchlist grab the option data
  # and union it to the watchlist_data df
  watchlist_data <- do.call(
    rbind,
    lapply(
      watchlist,
      grab_option_data
      )
    )
```

Let's check the result of our pull to see all of the unique tickers in the dataframe we created. It should be the same as the watchlist we created.

```{r}

print(unique(as.factor(watchlist_data$underlying_ticker)))

```

Pulling data for our whole watchlist produces about 49k rows.

```{r}
# number of rows for a single day's pull
nrow(watchlist_data)

```

### Creating a table in PostgreSQL (create_watchlist_table.R)

Now we know how to get the option data for every watchlist ticker for every expiration for every strike. But how do we get this into our PostgreSQL dataframe? First we load the `RPostgreSQL` library and call the `grab_watchlist.R` script containing our `grab_watchlist()` function.

```{r}

library(RPostgreSQL)

source("/home/sebi/optionsBacktesting/grab_watchlist.R")
```

Next we need to declare our driver name, the name of our database, the host (I am running this markdown from the Raspberry Pi, hence `localhost`), and the port we wish to connect through. Normally the user would have to enter their username and password to access the database, but I can't stand around each day waiting to enter my password, nor am I going to put my credentials on the screen for you monsters, so I need to automate this too. Luckily, the [PostgreSQL documentation](https://www.postgresql.org/docs/current/libpq-pgpass.html){target="_blank"} very clearly shows how to set up a `.pgpass` file on the Raspberry Pi. Note that this is for local PostgreSQL access which I intend to automate. This can only be manually accessed by a user who would have logged in by entering their credentials. I will still require logging remotely to require manually entering the user and password at this point, but we will get to that later.

```{r eval=FALSE}

# set driver name
driver_name <- dbDriver(drvName = "PostgreSQL")
  
# establish database connection
db <- DBI::dbConnect(driver_name,
		     dbname="sebi",
		     host="localhost",
		     port = 5432
		     )
```


We run our `grab_watchlist()` function to grab today's data.


```{r eval=FALSE}

# grab todays watchlist data
watchlist_data <- grab_watchlist()

```  
  
We only intend to build this table once, for the following days we will append to this table. However, we do want to standardize this procedure. The script to create a table will also allow overwriting of existing data. We do not need to worry about defining the schema (will be saved in `public`) nor whether a column is `VARCHAR(9)` or `VARCHAR(10)`. `R` will handle all of this for us.
  
```{r eval=FALSE}

# create our table with todays data (overwrite if already exists)
DBI::dbWriteTable(
  db,
  value =  watchlist_data,
  name = "watchlist_data",
  overwrite = TRUE,
  row.names = FALSE
)
```

We do need to define a primary key for the table, which we created from a combination of the option string and `scrape_date` earlier.

```{r eval=FALSE}  
# set primary key column
DBI::dbSendQuery(
    db,
    'ALTER TABLE watchlist_data ADD PRIMARY KEY ("option")'
    )
```

Let's be polite and disconnect from our database.

```{r eval=FALSE}
  # disconnect from database
  DBI::dbDisconnect(db)

```

### Appending to the existing PostgreSQL table (append_watchlist_data.R)

```{r}

library(DBI)
library(RPostgreSQL)

source("/home/sebi/optionsBacktesting/grab_watchlist.R")

```

The append script looks pretty much the same, except with `append = TRUE` instead of `overwrite = TRUE`. This is the script that will run every day.

```{r eval=FALSE}

append_watchlist_data <- function(){
  
  # grab our watchlist data
  watchlist_data <- grab_watchlist()
  
  # establish driver name
  driver_name <- dbDriver(drvName = "PostgreSQL")
  
  # create database connection
  db <- DBI::dbConnect(driver_name,
                       dbname="sebi",
                       host="localhost",
                       port = 5432
  )
  
  # append our scraped data to the table
  DBI::dbWriteTable(
    db,
    name = "watchlist_data",
    value = watchlist_data,
    row.names = FALSE,
    append = TRUE
  )
  
  # close database connection
  DBI::dbDisconnect(db)
}

```

### Only scrape for data on market days (trading_day_scheduler.R)

If we were to scrape this page on weekends, we would get the same result on Saturday as Friday because the market is closed. The same applies for market holidays. To prevent unnecessary duplicates, we only call this script when it is a weekday and not a holiday.

```{r}

library(timeDate)

source("/home/sebi/optionsBacktesting/append_watchlist_data.R")

if(as.POSIXlt(Sys.Date())$wday %in% 1:5 & !(Sys.Date() %in% as.Date(timeDate::holidayNYSE()))){
  append_watchlist_data()
} else {
  message("Market Closed Today")
}

```

### Setting up a cronjob to automate data scrapes

Automating the task is as simple as setting up a `cron` job. Make sure to do this as the same user that as you intend to intend to launch the scripts. For the sake of not poking the bear we will only run this once a day. The beginning of our line  `31 22 * * *` means that I want the job to run at every day at 10:31 PM for two reasons:

- The market will be closed
- 10:31 PM is when I stopped messing up the command.

The second part of our `cron` job tells our Raspberry Pi what to do at that time. In this case, first change directory (`cd /home/sebi/optionsBacktesting`) , then run the `R` script `trading_day_scheduler.R` to see if the market is open and add data to the PostgreSQL database if it is(`Rscript trading_day_scheduler.R`). The final part (`> log.txt 2>&1`) sends any terminal information to a log file we created.

```{r}

system(
  "sudo crontab -u sebi -l",
  intern = TRUE
)

```

We've made a few scripts so far. Let's take a look at all of our files again.

```{r}


system(
  "ls -al /home/sebi/optionsBacktesting/",
  intern = TRUE
)

```

### Performing an intial query from our database

We've reached the end of the first phase. But you're not into data if your natural sense of curiosity doesn't start to tingle. Let's do an initial pull of data from our repository and build a visualization. We will use the package `getPass` to prompt the user to enter their credentials before accessing the database.

```{r}

library(getPass)
library(ggplot2)
library(lubridate)
library(reshape2)
library(wesanderson)
library(RColorBrewer)

# set driver name
driver_name <- dbDriver(drvName = "PostgreSQL")

# establish database connection
db <- DBI::dbConnect(driver_name,
                     dbname = "sebi",
                     host = "192.168.0.12",
                     port = 5432,
                     user = "sebi",
                     password = getPass("Enter Password:")
)

res <- dbSendQuery(db, "SELECT * FROM watchlist_data;")
data_pull <- dbFetch(res)
dbClearResult(res)
dbDisconnect(db)

```

Let's just look at the most recent day's data.

```{r}
# make a vector of the tickers from the data fetch
tickers <- as.character(unique(as.factor(data_pull$underlying_ticker)))

# grab just the options from the most recent pull
recent_pull <- data_pull[data_pull$scrape_date == max(data_pull$scrape_date),]

# a blank dataframe to hold our put call ratios
pcr_list <- data.frame(matrix(ncol = length(tickers), nrow = 1))
colnames(pcr_list) <- tickers

print(pcr_list)

```


Now let's calulate the volume of puts divided by the volume of calls for each ticker (the put-call ratio). The higher the number is, the more bearish sentiment was in the market that day (more puts than calls). A number higher than 0.7 is generally considered neutral-to-bearish. 

```{r}

# calculate put call ratio for each ticker and add it to the put call ratio data frame
lapply(
  1:length(tickers),
  function(x){
    column_name <- tickers[x]
    if(sum(recent_pull[recent_pull$underlying_ticker == column_name & recent_pull$contract_type == "C","volume"]) != 0){
      pcr_list[,column_name] <<- sum(recent_pull[recent_pull$underlying_ticker == column_name & recent_pull$contract_type == "P","volume"]) / sum(recent_pull[recent_pull$underlying_ticker == column_name & recent_pull$contract_type == "C","volume"])
    } else {
        pcr_list[,column_name] <<- NA
      }
  }
)

print(pcr_list)

```


Right away we can create a tool more useful than what I can currently get from my Fidelity Active Trader Pro software. Instead of just displaying the put-call ratio for a single underlying, we can do this for every underlying in our watchlist. Since our watchlist consists of ETFs covering certain sectors and asset classes this can give us a pretty good picture of what is going on in the market today. First let's melt our data.

```{r}


# melt our data for our plot
melted_pcr_list <- reshape2::melt(
  pcr_list[, colSums(is.na(pcr_list)) < nrow(pcr_list)],
  variable.name = "Underlying",
  value.name = "put_call_ratio"
)

# turn the underlying ticker into a factor
melted_pcr_list$Underlying <- factor(
  melted_pcr_list$Underlying, 
  levels = melted_pcr_list$Underlying[order(melted_pcr_list$put_call_ratio)]
  )

print(melted_pcr_list)

```

Next let's plot our data as a bar chart so we can really see the differences. I used the `Rushmore1` palette from the `wesanderson` package because I am a shameless millennial. Until the next post, "I'll just go back out the window."

```{r}

ggplot(
  data =melted_pcr_list,
  aes(
    y = Underlying,
    x = put_call_ratio,
    fill = Underlying
  )
) +
  geom_bar(stat = "identity") +
  ggtitle("Watchlist Put-Call Ratios") +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_fill_manual(values = colorRampPalette(wes_palettes$Rushmore1)(nrow(melted_pcr_list))) +
  xlab("Put Call Ratio")
  

```
